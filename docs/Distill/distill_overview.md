# /distill Feature Overview

With the `/distill` feature, Distillery users have the ability to teach new subjects to our service and use them in any of the existing, powerful workflows. This is achieved by the simplest LoRA training implementation possible:

- Users provide the link of the image that needs to be trained.
- The distill command is launched.
- In a few minutes, the user is notified that their LoRA is ready to be used along with all the information needed to use it efficiently.

We already had custom LoRAs available on Distillery ([link to LoRA page](../Parameters/lora/lora.md)) and this feature extends the few hundred custom LoRAs to the possible infinite amount created by users.

Our first training service comes in the simplest possible form - it just needs a single image. It is an efficient and powerful method optimized for simplicity and clarity.

!!! note
    Please note that this is an early version of the service, expect constant updates, optimizations, and introduction of additional training options.

- To get the basics of how to use the `/distill` command, check out [this link](distill_command.md).
- To learn all about single image training, use cases, and tips and tricks on how to achieve optimal results, check out [this link](single_image.md).
- To learn how to manage your LoRAs, check out the list of existing ones, recall training details, and manage access levels, follow this [link](manage_lora.md).
- To learn more about pricing of LoRA training and all related details - check out [this link](distill_pricing.md).
